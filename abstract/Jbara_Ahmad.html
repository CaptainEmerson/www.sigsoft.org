<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>SIGSOFT: Abstract</title><link href="/sigsoft.css" rel="stylesheet" type="text/css"/></head><body><p>Program comprehension is the process of building a mental model of a given source code. It lies at the basis of any software maintenance activity such as fixing bugs and adding new features.  Maintenance is important because it consumes a large part of the resources allocated along a software lifecycle. Therefore, program comprehension is also important not only for the fact that it is a preliminary vital step in maintenance, but also for its criticality for the success of this activity. In particular, the better the code is understood the more likely the maintenance will be successful.  However, comprehension is directly affected by complexity; the less complex is the code the easier a programmer can understand it. For example, nesting and non-linear flow are factors that affect complexity and probably make code harder to comprehend. Therefore, we wish to enhance comprehension by making programs less complex. This would be feasible if we could measure the complexity of programs. As summarized by H.James Harrington, &#8220;measurement is the first step that leads to control and eventually to improvement. If you cannot measure something, you cannot understand it. If you cannot understand it, you cannot control it. If you cannot control it, you cannot improve it.&#8221; Over the years very many complexity metrics have been suggested. The large number of complexity metrics is probably an indication of a real difficulty in defining an ideal metric that is capable of reflecting complexity by providing one simple number.  The key problem of many existing metrics is that they estimate complexity by analyzing code syntax. For example, the McCabe&#8217;s cyclomatic complexity (MCC) is based on the number of independent execution paths in the code, which is equivalent to the number of conditions plus one. This metric is the most widely used since its introduction in 1976. Despite its popularity, it has been criticized over the years especially for the fact that it discards data flow and focuses on just counting program elements without paying attention to their context in the code.  The dissatisfaction with the state of the practice leads to the research question of why MCC and other metrics are not good enough to reflect effective complexity? what do these metrics miss and what is needed to define better metrics?  In this thesis, we introduce regularity as an additional factor that affects complexity. Specifically, regular code has many repetitions of a pattern, and successive instances become easier to comprehend given the experience with previous ones.  The innovation of regularity is introducing context awareness and sensitivity: a piece of code can have different complexity depending on neighboring code.  Interchangeably, code complexity is no longer absolute but depends on the context.  In particular, in regularity, the initial instances of some patterns have higher complexity than those that appear later in the code as the effective complexity of the repeated instances is reduced due to leveraging the understanding gained in the initial instances.  Conversely, the current metrics, including MCC, unconsciously neglect code context. Specifically, they simply count code elements. For example, the lines of code (LOC) metric count lines and MCC counts conditions.  It is important however to adopt an empirical approach to explore what effects exist in practice. In particular, we conducted a family of diverse experiments that encompass a wide range of experiments starting with a very basic subjective ranking up to very sophisticated eyetracking based experiments. The results show that subjects sometimes estimated functions with very high cyclomatic complexity as not complex. Moreover, they performed comprehension tasks and achieved better results in regular code (despite having higher cyclomatic complexity) when compared with its non-regular counterpart.  Beyond the immediate effects we already presented, our empirical investigation revealed more insights about reading in general and reading regular code in particular. Specifically, the results show that code reading is very non-linear as opposed to reading in natural language text. As for regular code, it seems that it is not read completely where parts of it are only scanned.  Having considered the way programmers read regular code, this led to an event-based framework for analyzing code reading. In particular, we extended the set of events (e.g, reading, scanning) that have been suggested in previous studies and suggested a way of coding these events for further analysis.  It is true that in this work we introduce a new property that affects complexity and challenges existing metrics, but regularity is just an example of such a factor and does not solve everything. It introduces new and wider considerations that can serve as good guidelines for investigating new factors.  This thesis is structured as a collection of papers that introduced and analyzed the above ideas. The first paper examines the state of the practice of real functions with very high cyclomatic complexity and it serves as a motivation paper for further work in the same direction [1]. This work was extended to a journal paper where the same idea was examined in more real systems trying to draw a general picture [2]. As high cyclomatic functions are also very long we suggested a visualization tool that helps among others in regularity identification [3].  The next step was to investigate the effect of regularity on comprehension by experimentally comparing regular and non-regular implementations of the same real problem. The results show that regular code is easier to comprehend despite being longer and more complex (according to it cyclomatic complexity) [4].  Another significant aspect we have examined is the way programmers read regular code. This yielded a model that reflects the decreasing invested effort in regular code [5]. Moreover, in its extended journal version we argue that code reading is largely different from natural language reading [6].  Finally, we suggested a way to measure regularity by means of compression.  It is a report that was not published yet [7].</p></body></html>