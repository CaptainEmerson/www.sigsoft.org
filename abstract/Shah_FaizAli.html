<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>SIGSOFT: Abstract</title><link href="/sigsoft.css" rel="stylesheet" type="text/css"/></head><body>
 
<p>With the advent of smartphones, tablets, and other mobile devices, the user base of software apps has grown tremendously and thus capturing the needs and expectations of such diverse user groups is not straightforward for developers during the mobile development release cycle. To handle such a complex situation, text mining techniques have been adopted to distill useful information automatically from a large volume of user reviews submitted to app marketplaces. In this thesis, first, we evaluate simple and complex review classification models for finding useful information in user reviews. Then, automatic app feature extraction techniques for fine-grained analysis of user reviews are investigated. Finally, both review classification and automatic app feature extraction techniques are combined to develop a tool for competitive analysis.</p>
<p>For automatic review classification, we evaluated the performances of simple to more complex machine learning models. Classification models using words in the review text as features called Bag-of-Words (BoW) are easier to adapt to other languages than the models using linguistic features because extracting linguistic features requires language-specific NLP tools. On the other hand, deep learning Convolutional Neural Network (CNN) architectures are more complex and harder
to interpret than the models using BoW and linguistic features but they do not require manual feature engineering efforts. The results of our experiments show that the simple BoW model can achieve almost the same performance as the more complex models using rich linguistic features or CNN architecture.  The task of automatic app feature extraction is challenging because of the informal nature of the review texts and the variability of the natural language.</p>
<p>Several methods including rule-based, unsupervised machine learning, and supervised machine learning, have been proposed for extracting app features from user reviews. However, these methods have used either different labeled datasets or different evaluation methods making their performances uncomparable. To establish a baseline performance, we evaluated the app feature extraction performance of existing rule-based and supervised machine learning methods in the same experimental setting. Our results show that the performances of both app feature extraction methods were low but the performance of a supervised learning method was better than the rule-based method in terms of f1-score. The performance of supervised machine learning methods depends on various aspects such as evaluation method, feature extraction methods, annotated review dataset and the guidelines used for the annotation of app features in a review dataset, also called annotation guidelines (AGs). Although AGs and the size of annotated datasets can potentially have a large effect on the evaluation results of supervised feature extraction methods and their usefulness, their impact on the performance of supervised app feature methods has been overlooked. To close this gap, we first analyze how the simulated application of AGs impacts the performance of a supervised machine learning method used to extract app features from user reviews. Then we explore the impact of the size of annotated datasets on app feature extraction performance.</p>
<p>Feature-level analysis performed on the reviews of a single app can also be extended to multiple apps for competitive analysis. We propose an approach that combines review classification and app feature extraction methods for comparing competing apps. To validate our approach, we developed the proof-of-concept tool REVSUM supports three typical use cases, i.e., viewing users' sentiments toward app features in competing apps (UC 1), viewing features that were mentioned in reviews classified as bug reports in competing apps (UC 2), and viewing features that were requested by users in competing apps (UC 3). In a followup qualitative evaluation, developers from industry have found REVSUM a useful tool for extracting information from app reviews of competing apps that is relevant for software maintenance and release planning. In summary, the thesis has explored existing review classification and app feature extraction techniques for finding developer-relevant information from user reviews and then combined these approaches to develop the tool REVSUM for comparing competing apps to support developers in software development activities.</p>
</body></html>
